'''Sync Method and Sync Location base classes'''

# allow class name being used before being fully defined (like in same class):
from __future__ import annotations

import json
import base64

from kiss_cf import logging
from .storage_master import StorageMaster, DerivingStorageMaster
from ._meta_data import MetaDataStorable


class KissStorageSyncException(Exception):
    ''' General exception from storage Sync '''


class KissChangeOnBothSidesException(Exception):
    ''' Files were changed on both storage locations sides when executing
    synchronization. '''


class SyncData(dict):
    ''' Synchronization data (as in: <some file>.sync)

    Synchronization data is stored in human readable JSON files to allow manual
    inspection.

    Data in ['location'] is with respect to the synchronization partner. When
    looking at file.sync in location A and seeing a UUID for location B, this
    implies: the latest sync between A and B was done based on the file state
    with this UUID.
    '''
    def __init__(self):
        super().__init__()
        # version for potential later upgrade handling:
        self.__setitem__('version', 1)
        # Locations (by their ID) with latest timestamp of this file.
        self.__setitem__('location', {})

    @classmethod
    def _get_location_template(cls) -> dict:
        ''' Template for location data '''
        # IMORTANT: if you change details here, you might need to update the
        # version in __init__ and handle for compatibility.
        return {
            # UUID is the main decision criterium for the sync algorithm.
            'uuid': '',
            # Timestamp was origingally intended to be used but is not reliable
            # in cases where updates/sync happen within seconds (as visible in
            # automated test cases). It is still kept and maintained since it
            # is valuable for manual inspections.
            'timestamp': None
            }

    # def set_location_timestamp(self,
    #                           location: StorageLocation,
    #                           timestamp: datetime):
    #    ''' Set last seen timestamp of this file in other location. '''
    #    str_timestamp = timestamp.isoformat()
    #    loc_id = location.get_id()
    #    if loc_id not in self['location']:
    #        self['location'][loc_id] = self._get_location_template()
    #    self['location'][loc_id]['timestamp'] = str_timestamp

    # Method excluded from coverage since it is currently unused in KISS_CF and
    # not intended for usage outside of KISS_CF. See the comment in
    # _get_location_template() on timestamp.
    # def get_location_timestamp(self,
    #                           location: StorageLocation
    #                           ) -> datetime | None:
    #    ''' Get last seen timestamp of this file in other location.
#
    #    Returns None if location is not known within this sync file.
    #    '''
    #    loc_id = location.get_id()
    #    if loc_id not in self['location']:
    #        return None
    #    return datetime.fromisoformat(self['location'][loc_id]['timestamp'])

    def set_location_uuid(self, storage: StorageMaster, uuid: bytes):
        ''' Set UUID of file in other location. '''
        str_uuid = base64.b64encode(uuid).decode('utf-8')
        loc_id = storage.get_id()
        if loc_id not in self['location']:
            self['location'][loc_id] = self._get_location_template()
        self['location'][loc_id]['uuid'] = str_uuid

    def get_location_uuid(self, storage: StorageMaster) -> bytes:
        ''' Get UUID of file in other location.

        Returns b'' if location is not known within this sync file.
        '''
        loc_id = storage.get_id()
        if loc_id not in self['location']:
            return b''
        uuid_str = self['location'][loc_id]['uuid']
        return base64.b64decode(uuid_str)

    @classmethod
    def from_bytes(cls, data: bytes) -> SyncData:
        ''' Construct a SyncData object from bytes

        Correct bytes are generated by to_bytes()
        '''
        obj = cls()
        obj.update(json.loads(data.decode('utf-8')))
        return obj

    def to_bytes(self):
        ''' Get bytes that represent the sync state

        SyncState can be reconstructed via SyncData.from_bytes().
        '''
        json_out = json.dumps(self, indent=4, separators=(',', ': '))
        return bytes(json_out, encoding='utf-8')


log = logging.getLogger(__name__)


# TODO: sync is broken when it cannot rely on StorageLocation anymore. It
# needs:
#   1) Retrieve registered fiels and the StorageMethod. [OK] via StorageMaster
#   2) Retrieve the UUID which is written in StorageLocation upon store()
#   3) Retrieve the timestamp which is a specialty of a StorageLocation
#      implementation.
#
# Alternative A: Traverse the aggregation history to see if there is a
# LocationStorage anywhere.
#
#  * (-) That would need access of private members. When made public, the
#    abstraction may get lost.
#
# Alternative B: At least UUID may be written elsewhere.
#
#  * (/) Writing a UUID in scope of sync() would require hashing of the file
#    content during sync which is acceptable. Alternatively, a complete copy
#    could be stored.
#  * (/) Writing UUID could be an extension via StorageMethod and
#    StorageMaster. This is already the case! The files may exist. It's just
#    that there is no public interface to retrieve the UUID. One would need to
#    know the file extension. >> acceptable
#  * (/) Timestamp is currently not used but has to be a functionality of the
#    location. Like UUID, the timestamp could be written into a separate file
#    (possibly together with UUID) and then accessed from the file system. This
#    timestamping would ensure to use a common time source for the running
#    application such that sync() could even place missing timestamps.
#
# Alternative B1: The Factory already "registers" to support sync. It will
# automatically add a StorageMethod inbetween that listens to the factories
# "part of a sync" property to enable writing meta data (timestamp, UUID, hash)
# upon store/load.

# Pyhon dirsync:
# https://github.com/tkhyn/dirsync/blob/develop/dirsync/syncer.py
#  * Uses timestamps (stat)
#  * Uses filecmp (stat: type, size, modification time; plus eventually
#    content)

# TODO: How does the sync consider removing files again??


def sync(loc_a: StorageMaster | DerivingStorageMaster,
         loc_b: StorageMaster | DerivingStorageMaster):
    ''' Synchronize StorageLocations with registered Files

    Check timestamps of files on both locations and forward to the refined
    SyncMechanism.sync(). Files on the remote location that do not match a
    Storable will be ignored.
    '''
    # TODO: add proper get_registered_files() interface to StorageLocation
    file_list = set(
        list(loc_a.get_file_list()) +
        list(loc_b.get_file_list()))
    log.debug(f'Starting sync between {loc_a} and {loc_b}')

    # ## Decision Stage 1: File Existance
    for file in file_list:
        storage_a = loc_a.get_storage(file)
        storage_b = loc_b.get_storage(file)
        local_meta = loc_a.get_meta_data(file)
        remote_meta = loc_b.get_meta_data(file)
        local_exists = storage_a.exists()
        remote_exists = storage_b.exists()
        if not local_exists and not remote_exists:
            # can happen if file was not created, yet
            log.debug(f'File {file} not existing on both sides')
            continue
        if not remote_exists:
            log.debug(f'File {file} not existing on {loc_b}')
            _sync_file(file, loc_a, loc_b, local_meta, remote_meta)
            continue
        if not local_exists:
            log.debug(f'File {file} not existing on {loc_a}')
            _sync_file(file, loc_b, loc_a, remote_meta, local_meta)
            continue
        # Both files exist. We continue normally.

        # ## Decision Stage 2: Decision based on UUID
        # Get file uuid's

        # read sync data
        local_sync_data = _load_sync_data(file, loc_a)
        remote_sync_data = _load_sync_data(file, loc_b)
        # timestamps and uuid in sync data
        last_local_uuid = local_sync_data.get_location_uuid(loc_b)
        last_remote_uuid = remote_sync_data.get_location_uuid(loc_a)

        # Defensive implementation: this case cannot happen.
        # StorageLocations should generate a UUID even if the file
        # initially has none.
        if (not last_remote_uuid or
                not last_local_uuid):
            raise KissStorageSyncException(
                f'[{file}] exists on both locaions but StorageLocation did '
                f'not return a UUID. This should not happen. Please '
                f'reconsider the StorageLocation implementation. '
                f'You could alternatively remove the file from one of the '
                f'locations')
        if (local_meta.uuid != last_local_uuid and
                remote_meta.uuid != last_remote_uuid):
            raise KissChangeOnBothSidesException(
                f'[{file}] changed on both sides. Not yet supported.')
        if local_meta.uuid != last_local_uuid:
            _sync_file(file, loc_a, loc_b, local_meta, remote_meta)
            # logging in _sync_file
        elif remote_meta.uuid != last_remote_uuid:
            _sync_file(file, loc_b, loc_a, remote_meta, local_meta)
            # TODO: this A/B and local/remote has to be straightened up.
            # logging in _sync_file
        else:
            log.debug(f'File {file} did not change.')


def _sync_file(file,
               source: StorageMaster | DerivingStorageMaster,
               target: StorageMaster | DerivingStorageMaster,
               source_meta: MetaDataStorable,
               target_meta: MetaDataStorable):

    log.info(f'Updating {file} from {source} to {target}')

    # TODO UPGRADE: mark files "not readable" during sync
    # self.__mark_file_in_sync

    # get data
    data = source.get_storage(file, create=False).load()
    # source_timestamp = source._get_location_timestamp(file)
    # source_uuid = source.get_uuid(file)
    source_sync_data = _load_sync_data(file, source)

    # write data
    target.get_storage(file, create=False).store(data)
    # update meta data:
    target_meta.uuid = source_meta.uuid
    target_meta.store()
    # target_timestamp = target._get_location_timestamp(file)
    target_sync_data = _load_sync_data(file, target)

    # update source sync data:
    # source_sync_data.set_location_timestamp(target, target_timestamp)
    source_sync_data.set_location_uuid(target, source_meta.uuid)
    _store_sync_data(file, source, source_sync_data)
    # update target sync data: source location must now contain timestamp
    # of newly written data
    # target_sync_data.set_location_timestamp(source, source_timestamp)
    target_sync_data.set_location_uuid(source, source_meta.uuid)
    _store_sync_data(file, target, target_sync_data)

    # TODO UPGRADE: unmark files "not readable"
    # self.__mark_sync_done(file, data)


def _load_sync_data(file,
                    storage: StorageMaster | DerivingStorageMaster
                    ) -> SyncData:
    # get sync storage
    if isinstance(storage, DerivingStorageMaster):
        file_storage = storage.get_root_master().get_storage(
            file + '.sync', register=False)
    else:
        file_storage = storage.get_storage(
            file + '.sync', register=False)
    # catch never synced case:
    if not file_storage.exists():
        return SyncData()
    # load data:
    raw_data = file_storage.load()
    # TODO: this behavior is quite poor. Something should be a Storable, here.
    return SyncData.from_bytes(raw_data)


def _store_sync_data(file,
                     storage: StorageMaster | DerivingStorageMaster,
                     sync_data: SyncData):
    # get sync storage
    if isinstance(storage, DerivingStorageMaster):
        file_storage = storage.get_root_master().get_storage(
            file + '.sync', register=False)
    else:
        file_storage = storage.get_storage(
            file + '.sync', register=False)
    raw_data = sync_data.to_bytes()
    file_storage.store(raw_data)
